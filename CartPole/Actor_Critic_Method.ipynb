{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "import gym, os\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "env = gym.make(\"CartPole-v0\")  ### 或者 env = gym.make(\"CartPole-v0\").unwrapped 开启无锁定环境训练\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.001\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = Categorical(F.softmax(output))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor Model loaded\n",
      "Critic Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_135484/3885703769.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  distribution = Categorical(F.softmax(output))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Score: 199\n",
      "Iteration: 10, Score: 199\n",
      "Iteration: 20, Score: 199\n",
      "Iteration: 30, Score: 199\n",
      "Iteration: 40, Score: 199\n",
      "Iteration: 50, Score: 199\n",
      "Iteration: 60, Score: 199\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/bear/work/gym/Actor_Critic_Method.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bear/work/gym/Actor_Critic_Method.ipynb#W2sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bear/work/gym/Actor_Critic_Method.ipynb#W2sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     critic \u001b[39m=\u001b[39m Critic(state_size, action_size)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bear/work/gym/Actor_Critic_Method.ipynb#W2sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m trainIters(actor, critic, n_iters\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m)\n",
      "\u001b[1;32m/home/bear/work/gym/Actor_Critic_Method.ipynb Cell 3\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(actor, critic, n_iters)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bear/work/gym/Actor_Critic_Method.ipynb#W2sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m optimizerA\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bear/work/gym/Actor_Critic_Method.ipynb#W2sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m optimizerC\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bear/work/gym/Actor_Critic_Method.ipynb#W2sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m actor_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bear/work/gym/Actor_Critic_Method.ipynb#W2sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m critic_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bear/work/gym/Actor_Critic_Method.ipynb#W2sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m optimizerA\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "\n",
    "def trainIters(actor, critic, n_iters):\n",
    "    optimizerA = optim.Adam(actor.parameters(), lr)\n",
    "    optimizerC = optim.Adam(critic.parameters(), lr)\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            # env.render()\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample([1])\n",
    "            next_state, reward, done, _ = env.step(action.cpu().squeeze(0).numpy()) \n",
    "\n",
    "            log_prob = dist.log_prob(action)\n",
    "            # entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float32).to(device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float32).to(device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                if iter % 10 == 0:\n",
    "                    print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = compute_returns(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.concat(log_probs)\n",
    "        returns = torch.concat(returns).detach()\n",
    "        values = torch.concat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n",
    "    torch.save(actor.state_dict(), 'model/actor.pdparams')\n",
    "    torch.save(critic.state_dict(), 'model/critic.pdparams')\n",
    "    env.close()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    if os.path.exists('model/actor.pdparams'):\n",
    "        actor = Actor(state_size, action_size).to(device)\n",
    "        model_state_dict  = torch.load('model/actor.pdparams')\n",
    "        actor.load_state_dict(model_state_dict )\n",
    "        print('Actor Model loaded')\n",
    "    else:\n",
    "        actor = Actor(state_size, action_size).to(device)\n",
    "    if os.path.exists('model/critic.pdparams'):\n",
    "        critic = Critic(state_size, action_size).to(device)\n",
    "        model_state_dict  = torch.load('model/critic.pdparams')\n",
    "        critic.load_state_dict(model_state_dict )\n",
    "        print('Critic Model loaded')\n",
    "    else:\n",
    "        critic = Critic(state_size, action_size).to(device)\n",
    "    trainIters(actor, critic, n_iters=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8dc982a7145738264708a45f37b8c78fa2fc664e1396a6c9770a92b61c37a37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
