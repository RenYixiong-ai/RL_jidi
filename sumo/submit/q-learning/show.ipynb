{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy\n",
    "import time\n",
    "import os\n",
    "CURRENT_PATH = os.getcwd()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from olympics_engine.scenario import wrestling\n",
    "from olympics_engine.generator import create_scenario\n",
    "gamemap =  create_scenario('wrestling')        #load map config\n",
    "game = wrestling(gamemap)\n",
    "obs = game.reset()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(obs[0]['agent_obs']) \n",
    "\n",
    "print(obs[1]['agent_obs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载专家策略\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.Conv = torch.nn.Conv2d(in_channels=1, out_channels=1 ,kernel_size=3, padding=1)\n",
    "        self.BN = torch.nn.BatchNorm2d(num_features=1)\n",
    "        self.Pool = torch.nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.NN1 = torch.nn.Linear(1600, 400)\n",
    "        self.NN2 = torch.nn.Linear(400, 128)\n",
    "        self.NN3 = torch.nn.Linear(128, 64)\n",
    "        self.NN4 = torch.nn.Linear(64, 2)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.Conv(input)\n",
    "        x = self.BN(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.Pool(x)\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = torch.flatten(input, start_dim=1)\n",
    "\n",
    "        x = self.NN1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.NN2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.NN3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.NN4(x)\n",
    "\n",
    "        x[:, 0] = torch.tanh(x[:, 0])*150 + 50\n",
    "        x[:, 1] = torch.tanh(x[:, 1])*30\n",
    "\n",
    "        return x\n",
    "\n",
    "expert = CNN().to(device)\n",
    "expert.load_state_dict(torch.load('./expert.pt'))\n",
    "expert.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置训练网络\n",
    "class agent(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(agent, self).__init__()\n",
    "        self.Conv = torch.nn.Conv2d(in_channels=1, out_channels=1 ,kernel_size=3, padding=1)\n",
    "        self.BN = torch.nn.BatchNorm2d(num_features=1)\n",
    "        self.Pool = torch.nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.NN1 = torch.nn.Linear(1600, 800)\n",
    "        self.NN2 = torch.nn.Linear(800, 128)\n",
    "        self.NN3 = torch.nn.Linear(128, 64)\n",
    "        self.NN4 = torch.nn.Linear(64, 2)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.Conv(input)\n",
    "        x = self.BN(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.Pool(x)\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = torch.flatten(input, start_dim=1)\n",
    "\n",
    "        x = self.NN1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.NN2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.NN3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.NN4(x)\n",
    "\n",
    "        x[:, 0] = torch.tanh(x[:, 0])*150 + 50\n",
    "        x[:, 1] = torch.tanh(x[:, 1])*30\n",
    "\n",
    "        return x\n",
    "\n",
    "model = agent().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pygame\n",
    "\n",
    "\n",
    "print('action dim 0 range = ', game.action_f)\n",
    "print('action dim 1 range = ', game.action_theta)\n",
    "our_team = 0        #we control team_0\n",
    "obs = game.reset()\n",
    "\n",
    "done = False\n",
    "RENDER = True\n",
    "\n",
    "if RENDER:\n",
    "    game.render()\n",
    "\n",
    "while not done:\n",
    "    #学习的策略\n",
    "    #action_team_0 = [random.uniform(-100,200), random.uniform(-30, 30)]     #for now we use random actions for both teams\n",
    "    #action_team_0 = [26, 0]\n",
    "    agent_obs = torch.tensor([[obs[0]['agent_obs']]], dtype=torch.float32)\n",
    "    agent_obs = agent_obs.to(device)\n",
    "    agent_action = model(agent_obs)[0].cpu()\n",
    "    action_team_0 = [agent_action[0].item(), agent_action[1].item()]\n",
    "\n",
    "\n",
    "    #专家策略给的行动\n",
    "    expert_obs = torch.tensor([[obs[1]['agent_obs']]], dtype=torch.float32)\n",
    "    expert_obs = expert_obs.to(device)\n",
    "    expert_action = expert(expert_obs)[0].cpu()\n",
    "    action_team_expert = [expert_action[0].item(), expert_action[1].item()]\n",
    "\n",
    "    input_action = [action_team_0, action_team_expert]\n",
    "\n",
    "    next_obs, reward, done, _ = game.step(input_action)\n",
    "    if RENDER:\n",
    "        time.sleep(0.05)\n",
    "        game.render()\n",
    "    \n",
    "    obs = next_obs\n",
    "print('final reward = ', reward)\n",
    "pygame.display.quit()\n",
    "#pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('RL_jidi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56aa5d81d4959884d9009b0c5239d0923ad1f8132d8ef306540209260b17049c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
