{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "import gym, os\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "env = gym.make(\"CartPole-v0\")  ### 或者 env = gym.make(\"CartPole-v0\").unwrapped 开启无锁定环境训练\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "\n",
    "def trainIters(actor, critic, n_iters):\n",
    "    optimizerA = optim.Adam(actor.parameters(), lr)\n",
    "    optimizerC = optim.Adam(critic.parameters(), lr)\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            # env.render()\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "            dist, value = actor(state), critic(state)   #演员的动作，评论家给予当前状态的价值\n",
    "\n",
    "            action = dist.sample([1])\n",
    "            next_state, reward, done, _ = env.step(action.cpu().squeeze(0).numpy()) \n",
    "\n",
    "            log_prob = dist.log_prob(action)        #返回该动作的可能性\n",
    "            # entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)                    \n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float32).to(device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float32).to(device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                if iter % 10 == 0:\n",
    "                    print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "        next_value = critic(next_state)             # 评论家对结局打分\n",
    "        returns = compute_returns(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.concat(log_probs)\n",
    "        returns = torch.concat(returns).detach()\n",
    "        values = torch.concat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n",
    "    torch.save(actor.state_dict(), 'model/actor.pdparams')\n",
    "    torch.save(critic.state_dict(), 'model/critic.pdparams')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1-False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8dc982a7145738264708a45f37b8c78fa2fc664e1396a6c9770a92b61c37a37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
